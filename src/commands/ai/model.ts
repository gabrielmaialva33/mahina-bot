import Command from '#common/command'
import type Context from '#common/context'
import type MahinaBot from '#common/mahina_bot'
import { ApplicationCommandOptionType, EmbedBuilder } from 'discord.js'

export default class Model extends Command {
  constructor(client: MahinaBot) {
    super(client, {
      name: 'model',
      description: {
        content:
          'Gerencie modelos de IA da NVIDIA - incluindo Llama 4 Maverick e outros modelos avan√ßados',
        examples: [
          'model',
          'model select llama-4-maverick',
          'model info deepseek-r1',
          'model stats',
        ],
        usage: 'model [subcommand] [model]',
      },
      category: 'ai',
      aliases: ['aimodel', 'setmodel', 'models'],
      cooldown: 3,
      args: false,
      vote: false,
      player: undefined,
      permissions: {
        client: ['SendMessages', 'ViewChannel', 'EmbedLinks'],
        user: [],
      },
      slashCommand: true,
      options: [
        {
          name: 'list',
          description: 'Listar todos os modelos de IA dispon√≠veis',
          type: ApplicationCommandOptionType.Subcommand,
          options: [
            {
              name: 'category',
              description: 'Filtrar por categoria',
              type: ApplicationCommandOptionType.String,
              required: false,
              choices: [
                { name: 'üåü Multimodal', value: 'multimodal' },
                { name: 'üß† Racioc√≠nio', value: 'reasoning' },
                { name: 'üëÅÔ∏è Vis√£o', value: 'vision' },
                { name: 'üí¨ Geral', value: 'general' },
                { name: 'üíª C√≥digo', value: 'coding' },
              ],
            },
          ],
        },
        {
          name: 'select',
          description: 'Selecionar um modelo de IA',
          type: ApplicationCommandOptionType.Subcommand,
          options: [
            {
              name: 'model',
              description: 'O modelo para selecionar',
              type: ApplicationCommandOptionType.String,
              required: true,
              choices: [
                { name: 'üß† DeepSeek R1 (Racioc√≠nio)', value: 'deepseek-r1' },
                { name: 'üí¨ Llama 3.3 70B', value: 'llama-70b' },
                { name: 'üì° Llama 3.1 70B (Stream)', value: 'llama-70b-stream' },
                { name: '‚ö° Llama 3 8B', value: 'llama3-8b' },
                { name: 'üëÅÔ∏è Llama 3.2 11B Vision', value: 'llama-3.2-11b-vision' },
                { name: 'üîç Llama 3.2 90B Vision', value: 'llama-3.2-90b-vision' },
                { name: 'üíª Qwen 2.5 Coder 32B', value: 'qwen-coder' },
                { name: 'üéØ Qwen2 7B Instruct', value: 'qwen2-7b' },
                { name: 'üñ•Ô∏è Code Llama 70B', value: 'codellama-70b' },
                { name: 'üöÄ Mistral Large 2', value: 'mistral-large-2' },
                { name: '‚öôÔ∏è Mixtral 8x7B', value: 'mixtral-8x7b' },
                { name: 'üß© Mixtral 8x22B', value: 'mixtral-8x22b' },
                { name: 'üí° Codestral 22B', value: 'codestral-22b' },
                { name: 'üì± Phi-3 Medium 128K', value: 'phi-3-medium' },
                { name: '‚ö° Phi-3 Small 128K', value: 'phi-3-small' },
                { name: 'üî∑ Gemma 2 27B', value: 'gemma-2-27b' },
                { name: 'üî∏ Gemma 2 9B', value: 'gemma-2-9b' },
                { name: 'üíª CodeGemma 7B', value: 'codegemma-7b' },
              ],
            },
          ],
        },
        {
          name: 'info',
          description: 'Obter informa√ß√µes sobre um modelo espec√≠fico',
          type: ApplicationCommandOptionType.Subcommand,
          options: [
            {
              name: 'model',
              description: 'O modelo para obter informa√ß√µes',
              type: ApplicationCommandOptionType.String,
              required: true,
              choices: [
                { name: 'üß† DeepSeek R1', value: 'deepseek-r1' },
                { name: 'üí¨ Llama 3.3 70B', value: 'llama-70b' },
                { name: 'üì° Llama 3.1 70B (Stream)', value: 'llama-70b-stream' },
                { name: '‚ö° Llama 3 8B', value: 'llama3-8b' },
                { name: 'üëÅÔ∏è Llama 3.2 11B Vision', value: 'llama-3.2-11b-vision' },
                { name: 'üîç Llama 3.2 90B Vision', value: 'llama-3.2-90b-vision' },
                { name: 'üíª Qwen 2.5 Coder 32B', value: 'qwen-coder' },
                { name: 'üéØ Qwen2 7B Instruct', value: 'qwen2-7b' },
                { name: 'üëÅÔ∏è Gemma 3 27B', value: 'gemma-3-27b' },
                { name: 'üåç Cosmos Predict 7B', value: 'cosmos-predict-7b' },
              ],
            },
          ],
        },
        {
          name: 'current',
          description: 'Mostrar seu modelo de IA atual',
          type: ApplicationCommandOptionType.Subcommand,
        },
        {
          name: 'stats',
          description: 'Ver estat√≠sticas de uso dos modelos',
          type: ApplicationCommandOptionType.Subcommand,
          options: [
            {
              name: 'period',
              description: 'Per√≠odo de tempo',
              type: ApplicationCommandOptionType.String,
              required: false,
              choices: [
                { name: '√öltima hora', value: '1h' },
                { name: '√öltimas 24 horas', value: '24h' },
                { name: '√öltimos 7 dias', value: '7d' },
                { name: '√öltimos 30 dias', value: '30d' },
              ],
            },
          ],
        },
      ],
    })
  }

  async run(client: MahinaBot, ctx: Context, args: string[]): Promise<any> {
    const subcommand = ctx.isInteraction
      ? ctx.options.getSubCommand() || 'current'
      : args[0]?.toLowerCase() || 'current'

    // Try enhanced service first, fallback to regular
    const nvidiaService = client.services.nvidiaEnhanced || client.services.nvidia

    if (!nvidiaService) {
      return await ctx.sendMessage({
        embeds: [
          {
            description: '‚ùå Servi√ßo de IA NVIDIA n√£o est√° configurado',
            color: client.config.color.red,
          },
        ],
      })
    }

    switch (subcommand) {
      case 'list': {
        const category = ctx.isInteraction
          ? (ctx.options.get('category')?.value as string)
          : args[1]

        if (nvidiaService.createEnhancedModelEmbed) {
          const embed = nvidiaService.createEnhancedModelEmbed()
          return await ctx.sendMessage({ embeds: [embed] })
        } else {
          const embed = nvidiaService.createModelEmbed()
          return await ctx.sendMessage({ embeds: [embed] })
        }
      }

      case 'select': {
        const modelKey = ctx.isInteraction ? (ctx.options.get('model')?.value as string) : args[1]

        if (!modelKey) {
          return await ctx.sendMessage({
            embeds: [
              {
                description: '‚ùå Por favor, especifique um modelo para selecionar',
                color: client.config.color.red,
              },
            ],
          })
        }

        const success = nvidiaService.setUserModel(ctx.author!.id, modelKey)

        if (!success) {
          return await ctx.sendMessage({
            embeds: [
              {
                description: `‚ùå Modelo inv√°lido: \`${modelKey}\``,
                color: client.config.color.red,
              },
            ],
          })
        }

        const model = nvidiaService.getModelInfo(modelKey)
        const embed = new EmbedBuilder()
          .setTitle('‚úÖ Modelo Selecionado')
          .setDescription(`Voc√™ agora est√° usando **${model!.name}**`)
          .setColor(client.config.color.green)
          .addFields(
            { name: 'Categoria', value: model!.category, inline: true },
            {
              name: 'Contexto',
              value: `${model!.contextLength.toLocaleString()} tokens`,
              inline: true,
            },
            {
              name: 'Streaming',
              value: model!.streaming ? '‚úÖ Habilitado' : '‚ùå Desabilitado',
              inline: true,
            }
          )
          .setFooter({ text: 'Use !chat ou /chat para conversar com o novo modelo!' })

        return await ctx.sendMessage({ embeds: [embed] })
      }

      case 'info': {
        const modelKey = ctx.isInteraction ? (ctx.options.get('model')?.value as string) : args[1]

        if (!modelKey) {
          return await ctx.sendMessage({
            embeds: [
              {
                description: '‚ùå Por favor, especifique um modelo',
                color: client.config.color.red,
              },
            ],
          })
        }

        const model = nvidiaService.getModelInfo(modelKey)

        if (!model) {
          return await ctx.sendMessage({
            embeds: [
              {
                description: `‚ùå Modelo n√£o encontrado: \`${modelKey}\``,
                color: client.config.color.red,
              },
            ],
          })
        }

        const embed = new EmbedBuilder()
          .setTitle(`ü§ñ ${model.name}`)
          .setDescription(model.description)
          .setColor(client.config.color.main)
          .addFields(
            { name: 'ID do Modelo', value: `\`${model.id}\``, inline: false },
            { name: 'Categoria', value: model.category, inline: true },
            {
              name: 'Contexto',
              value: `${model.contextLength.toLocaleString()} tokens`,
              inline: true,
            },
            { name: 'Max Tokens', value: `${model.maxTokens.toLocaleString()}`, inline: true },
            { name: 'Temperatura', value: `${model.temperature}`, inline: true },
            { name: 'Top P', value: `${model.topP}`, inline: true },
            {
              name: 'Streaming',
              value: model.streaming ? '‚úÖ Suportado' : '‚ùå N√£o Suportado',
              inline: true,
            }
          )

        // Add features if enhanced model
        if (model.features && model.features.length > 0) {
          embed.addFields({
            name: '‚ú® Recursos',
            value: model.features.map((f) => `‚Ä¢ ${f}`).join('\n'),
            inline: false,
          })
        }

        // Add cost if available
        if (model.costPerMillion) {
          embed.addFields({
            name: 'üí∞ Custo',
            value: `$${model.costPerMillion} por milh√£o de tokens`,
            inline: true,
          })
        }

        if (model.latency) {
          embed.addFields({
            name: '‚ö° Lat√™ncia',
            value: model.latency,
            inline: true,
          })
        }

        return await ctx.sendMessage({ embeds: [embed] })
      }

      case 'stats': {
        const period = ctx.isInteraction
          ? (ctx.options.get('period')?.value as string) || '24h'
          : args[1] || '24h'

        if (!nvidiaService.getModelStats) {
          return await ctx.sendMessage({
            embeds: [
              {
                description: '‚ùå Estat√≠sticas n√£o dispon√≠veis',
                color: client.config.color.red,
              },
            ],
          })
        }

        const stats = await nvidiaService.getModelStats(period)
        const embed = new EmbedBuilder()
          .setTitle(`üìä Estat√≠sticas de Uso (${period})`)
          .setColor(client.config.color.blue)
          .setDescription('M√©tricas de performance dos modelos de IA')
          .setTimestamp()

        if (Array.isArray(stats) && stats.length > 0) {
          for (const stat of stats.slice(0, 10)) {
            embed.addFields({
              name: stat.model_name,
              value: [
                `Requisi√ß√µes: **${stat.total_requests}**`,
                `Tokens: **${Number(stat.total_tokens).toLocaleString()}**`,
                `Tempo m√©dio: **${Math.round(stat.avg_response_time)}ms**`,
                `Taxa de sucesso: **${Math.round(stat.success_rate * 100)}%**`,
              ].join(' | '),
              inline: false,
            })
          }
        } else {
          embed.setDescription('Nenhuma estat√≠stica dispon√≠vel no momento.')
        }

        return await ctx.sendMessage({ embeds: [embed] })
      }

      case 'current':
      default: {
        const embed = nvidiaService.createModelStatusEmbed(ctx.author!.id)
        return await ctx.sendMessage({ embeds: [embed] })
      }
    }
  }
}
